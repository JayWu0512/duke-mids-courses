[![Python Template for IDS706](https://github.com/JayWu0512/duke-mids-courses/actions/workflows/ids706-ci.yml/badge.svg)](https://github.com/JayWu0512/duke-mids-courses/actions/workflows/ids706-ci.yml)

# LinkedIn Jobs & Skills Analysis

This project analyzes LinkedIn job postings and skills data using a structured data pipeline (Polars) and machine learning exploration (KMeans clustering).

## Project Structure

```
data/
├── raw/                      # Original Kaggle parquet files
│   ├── job_skills.parquet
│   ├── job_summary.parquet
│   └── linkedin_job_postings.parquet
├── bronze/                   # Cleaned + normalized
│   └── jobs.parquet
├── silver/                   # Role-filtered + text-joined
│   └── jobs_text.parquet
├── gold/                     # Aggregated skills & final outputs
│   └── top_skills.parquet
└── test/                     # Tiny test data (generated by script)
    ├── tiny_jobs.parquet
    ├── tiny_jobs_text.parquet
    └── tiny_top_skills.parquet

notebooks/
├── 01_eda.ipynb              # Data inspection & visualization
└── 02_kmeans.ipynb           # ML exploration (TF-IDF + clustering)

scripts/
├── download_kaggle.py        # Kaggle download & parquet conversion
├── make_test_data.py         # Generate stratified test parquet
└── test_file_test.py         # Example test script

src/
├── app/                      # Pipeline orchestration (Typer CLI)
│   ├── __init__.py
│   ├── cli.py
│   └── pipeline.py
├── domain/                   # Ports/abstractions
│   └── ports.py
├── infra/                    # IO adapters, transformers, aggregators
│   ├── __init__.py
│   ├── aggregators.py
│   ├── io_polars.py
│   └── transformers.py
└── utils/                    # Helpers & config
    ├── __init__.py
    ├── config.py
    └── settings.py

tests/
├── integration/
│   └── test_pipeline_smoke.py
├── ml/
│   └── test_tfidf_kmeans.py
├── unit/
│   ├── test_filter_and_derive.py
│   └── test_io_and_schema.py
└── conftest.py

.devcontainer/                # VS Code devcontainer config
  └── devcontainer.json

Dockerfile                    # Build project container
docker-compose.yml            # Orchestrate services (build + test)
Makefile                      # Common commands (install, test, clean)
requirements.txt              # Dependencies
pytest.ini                    # Pytest config
.flake8                       # Flake8 linting config
.gitignore                    # Git ignore rules
.dockerignore                 # Docker ignore rules
README.md                     # Project documentation
```

## Data Source
- Job postings dataset (CSV/Parquet) with fields including **title, company, location, work_type, seniority, and listed skills**.
- Dataset is filtered to a manageable slice for computation in the notebooks.

## Pipeline

The pipeline builds multiple layers of data:

1. **Raw** → Original Kaggle parquet files.  
2. **Bronze** → Cleaned and normalized schema.  
3. **Silver** → Role-filtered, text-joined job postings.  
4. **Gold** → Aggregated top skills.  

Run with:

```bash
make build
```

Outputs will be written into `data/bronze/`, `data/silver/`, and `data/gold/`.

## Notebooks

- **01_eda.ipynb**:  
  Data inspection, null analysis, column distributions, posting trends.  
  Example: work type breakdown (onsite, hybrid, remote).

- **02_kmeans.ipynb**:  
  Machine learning exploration with TF-IDF + KMeans clustering.  
  Includes elbow method to choose K and visualization of top TF-IDF terms per cluster.

## Analysis Steps

### 1. Exploratory Data Analysis (`01_eda.ipynb`)
- **Health checks** on missing values and duplicates.  
- **Distributions** of roles, work types, and seniority levels.  
- **Top locations & companies** by posting frequency.  
- **Time series analysis**: posting trend by month.  
- **Skills analysis**:  
  - Top skills overall  
  - Top skills by role  
  - Co-occurrence network of skills  

### 2. KMeans Clustering (`02_kmeans.ipynb`)
- Feature engineering: transforming skills & job features into vectors.  
- Running **KMeans clustering** to group similar jobs.  
- Evaluating clusters using **silhouette score**.  
- Visualizing clusters to interpret relationships among job postings.

## Visualization
- Bar charts for top skills, roles, and companies.  
- Line charts for posting trends over time.  
- Scatter plots of clusters in reduced dimensions.  
- Skill co-occurrence diagrams.  

## Insights
- Many job postings are missing work type and seniority information → imputation and text-based derivation improves coverage.  
- Distribution: majority of postings are "onsite", but remote/hybrid roles exist in smaller proportions.  
- KMeans clustering (TF-IDF on job text) reveals meaningful groupings:  
  - **Cluster 0**: software / embedded software roles.  
  - Other clusters capture data-focused roles (data scientist, analyst, engineer).  
- The elbow method suggested **K ≈ 6** as a reasonable trade-off for cluster coherence.

---

## Setup Instructions

You can run this project in multiple ways:

### 1. Local environment
```bash
make install
```
This installs all dependencies from `requirements.txt`.

### 2. Devcontainer (VSCode)
- Open the `IDS706/` folder in VSCode.  
- Reopen in **Dev Container** (uses `.devcontainer/devcontainer.json`).  
- Dependencies will be auto-installed.  

### 3. Docker (alternative)
Build and run inside Docker:

```bash
docker compose up --build
```

This will:
- Build the container using the `Dockerfile`  
- Run `scripts/make_test_data.py` to generate tiny test parquet files  
- Run `pytest` automatically  

---

## Running Tests

### Locally
```bash
make test
```

### In CI/CD
This project includes a GitHub Actions workflow (`.github/workflows/ids706-ci.yml`).  
Every push or pull request to `IDS706/` will:
- Install dependencies  
- (Optionally) lint code  
- Run unit and integration tests with coverage reporting  

---

## Project Goals and Results

- **Goal**: Build an end-to-end pipeline to analyze LinkedIn job postings and derive skill insights using modern data engineering (Polars) and ML clustering.  
- **Results**:  
  - Built a **multi-layer data pipeline (raw → bronze → silver → gold)**  
  - Successfully **normalized heterogeneous job postings** (different column names across sources)  
  - Derived **work type and seniority** when missing, increasing data coverage  
  - Extracted **top skills** by frequency and role  
  - Used **TF-IDF + KMeans** clustering to uncover patterns in job text  
  - Key insight: Clusters revealed distinct role families (software engineering, data/ML roles, business/analyst roles)  

These results demonstrate how structured data pipelines and ML techniques can complement each other in job market analysis.
